"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[577],{8453:(e,i,n)=>{n.d(i,{R:()=>o,x:()=>a});var r=n(6540);const t={},s=r.createContext(t);function o(e){const i=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:i},e.children)}},9260:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"neural-inference/neural-posterior","title":"Neural Posterior Estimation","description":"This section covers the role of neural networks in approximating posterior distributions within the Simulation-Based Inference (SBI) pipeline. Neural density estimators provide a scalable and flexible means of learning complex posterior shapes from simulation data.","source":"@site/docs/neural-inference/002-neural-network.md","sourceDirName":"neural-inference","slug":"/neural-inference/neural-posterior","permalink":"/likelihood-free-inference/neural-inference/neural-posterior","draft":false,"unlisted":false,"editUrl":"https://github.com/role-model/likelihood-free-inference/docs/neural-inference/002-neural-network.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"neural-posterior","title":"Neural Posterior Estimation","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Neural Posterior Estimation","permalink":"/likelihood-free-inference/category/neural-posterior-estimation"},"next":{"title":"GPU Acceleration","permalink":"/likelihood-free-inference/category/gpu-acceleration"}}');var t=n(4848),s=n(8453);const o={id:"neural-posterior",title:"Neural Posterior Estimation",sidebar_position:1},a="Neural Posterior Estimation",l={},d=[{value:"Purpose of Neural Posterior Estimators",id:"purpose-of-neural-posterior-estimators",level:2},{value:"Supported Architectures",id:"supported-architectures",level:2},{value:"1. Masked Autoregressive Flow (MAF)",id:"1-masked-autoregressive-flow-maf",level:3},{value:"2. Mixture Density Network (MDN)",id:"2-mixture-density-network-mdn",level:3},{value:"3. Neural Spline Flow (NSF)",id:"3-neural-spline-flow-nsf",level:3},{value:"Training the Estimator",id:"training-the-estimator",level:2},{value:"Conditioning and Sampling",id:"conditioning-and-sampling",level:2},{value:"Evaluation and Diagnostics",id:"evaluation-and-diagnostics",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const i={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"neural-posterior-estimation",children:"Neural Posterior Estimation"})}),"\n",(0,t.jsx)(i.p,{children:"This section covers the role of neural networks in approximating posterior distributions within the Simulation-Based Inference (SBI) pipeline. Neural density estimators provide a scalable and flexible means of learning complex posterior shapes from simulation data."}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"purpose-of-neural-posterior-estimators",children:"Purpose of Neural Posterior Estimators"}),"\n",(0,t.jsx)(i.p,{children:"In likelihood-free settings, we cannot compute or differentiate the likelihood. Neural posterior estimators address this by directly modeling the posterior ( p(\\theta | x) ), trained on synthetic pairs of parameters (( \\theta )) and observations (( x ))."}),"\n",(0,t.jsx)(i.p,{children:"These models learn a conditional distribution over parameters given observed data and can be queried for fast posterior sampling."}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"supported-architectures",children:"Supported Architectures"}),"\n",(0,t.jsxs)(i.p,{children:["The ",(0,t.jsx)(i.code,{children:"sbi"})," library supports several neural density estimators:"]}),"\n",(0,t.jsx)(i.h3,{id:"1-masked-autoregressive-flow-maf",children:"1. Masked Autoregressive Flow (MAF)"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Suitable for continuous, high-dimensional parameter spaces"}),"\n",(0,t.jsx)(i.li,{children:"Learns flexible, invertible transformations of a base distribution"}),"\n",(0,t.jsx)(i.li,{children:"Often used as the default in SNPE"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"2-mixture-density-network-mdn",children:"2. Mixture Density Network (MDN)"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Models the posterior as a mixture of Gaussians"}),"\n",(0,t.jsx)(i.li,{children:"Simple and interpretable, but less flexible for complex posteriors"}),"\n",(0,t.jsx)(i.li,{children:"May struggle with multimodality or high-dimensionality"}),"\n"]}),"\n",(0,t.jsx)(i.h3,{id:"3-neural-spline-flow-nsf",children:"3. Neural Spline Flow (NSF)"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"An advanced normalizing flow using rational quadratic splines"}),"\n",(0,t.jsx)(i.li,{children:"Provides highly expressive modeling power"}),"\n"]}),"\n",(0,t.jsx)(i.hr,{}),"\n",(0,t.jsx)(i.h2,{id:"training-the-estimator",children:"Training the Estimator"}),"\n",(0,t.jsxs)(i.p,{children:["The estimator is trained using parameter\u2013observation pairs produced by simulations. The training process is handled internally by the ",(0,t.jsx)(i.code,{children:"sbi"})," inference API:"]}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:"inference = SNPE(prior)\ndensity_estimator = inference.append_simulations(theta, x).train()\nposterior = inference.build_posterior(density_estimator)\n"})}),"\n",(0,t.jsx)(i.p,{children:"Internally, the network is optimized using negative log-likelihood loss. Training supports GPU acceleration and can be monitored via logging tools such as TensorBoard."}),"\n",(0,t.jsx)(i.h2,{id:"conditioning-and-sampling",children:"Conditioning and Sampling"}),"\n",(0,t.jsx)(i.p,{children:"Once trained, the posterior estimator can be conditioned on observed data to draw samples:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-python",children:"samples = posterior.sample((1000,), x=x_o)\n"})}),"\n",(0,t.jsxs)(i.p,{children:["The estimator returns samples from the learned posterior ( p(\u03b8) | x",(0,t.jsx)("sub",{children:"o"}),"), where ( x",(0,t.jsx)("sub",{children:"o"})," ) is the observation of interest."]}),"\n",(0,t.jsx)(i.h2,{id:"evaluation-and-diagnostics",children:"Evaluation and Diagnostics"}),"\n",(0,t.jsx)(i.p,{children:"It is essential to evaluate the performance of the neural posterior estimator. Recommended practices include:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:"Visual inspection of posterior samples"}),"\n",(0,t.jsx)(i.li,{children:"Posterior predictive checks"}),"\n",(0,t.jsx)(i.li,{children:"Comparison with known ground truth (if available)"}),"\n",(0,t.jsx)(i.li,{children:"Coverage tests using simulated data"}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:"If diagnostic plots or metrics indicate poor calibration or fit, consider increasing simulation budget, using alternative architectures, or tuning hyperparameters."}),"\n",(0,t.jsx)(i.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(i.p,{children:"Neural posterior estimation is the core machine learning component of the SBI pipeline. It enables efficient and expressive inference over complex posterior distributions without requiring access to the likelihood function."}),"\n",(0,t.jsx)(i.p,{children:"In the next section, we describe how to accelerate training and inference using CUDA-enabled GPUs."})]})}function u(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);