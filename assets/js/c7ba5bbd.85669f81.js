"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[473],{4113:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>s,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"acceleration/gpu-acceleration","title":"GPU Acceleration","description":"This section outlines how to enable and utilize GPU acceleration in the Simulation-Based Inference (SBI) pipeline. Leveraging CUDA-capable GPUs can significantly improve training performance, especially for large-scale simulations or high-dimensional parameter spaces.","source":"@site/docs/acceleration/003-sbi-gpu-acceleration.md","sourceDirName":"acceleration","slug":"/acceleration/gpu-acceleration","permalink":"/acceleration/gpu-acceleration","draft":false,"unlisted":false,"editUrl":"https://github.com/role-model/likelihood-free-inference/docs/acceleration/003-sbi-gpu-acceleration.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"gpu-acceleration","title":"GPU Acceleration","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"GPU Acceleration","permalink":"/category/gpu-acceleration"},"next":{"title":"Full Example","permalink":"/category/full-example"}}');var r=i(4848),a=i(8453);const l={id:"gpu-acceleration",title:"GPU Acceleration",sidebar_position:1},s="GPU Acceleration",o={},c=[{value:"Requirements",id:"requirements",level:2},{value:"Using the GPU in SBI Workflows",id:"using-the-gpu-in-sbi-workflows",level:2},{value:"Training with GPU",id:"training-with-gpu",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Batch Size and Memory",id:"batch-size-and-memory",level:3},{value:"Data Movement",id:"data-movement",level:3},{value:"Parallelism",id:"parallelism",level:3},{value:"Verifying GPU Usage",id:"verifying-gpu-usage",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"gpu-acceleration",children:"GPU Acceleration"})}),"\n",(0,r.jsx)(n.p,{children:"This section outlines how to enable and utilize GPU acceleration in the Simulation-Based Inference (SBI) pipeline. Leveraging CUDA-capable GPUs can significantly improve training performance, especially for large-scale simulations or high-dimensional parameter spaces."}),"\n",(0,r.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,r.jsx)(n.p,{children:"To use GPU acceleration, your system must meet the following conditions:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA GPU"})," with CUDA support"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CUDA Toolkit"})," installed (version compatible with PyTorch)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PyTorch with CUDA backend"})," installed"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You can install PyTorch with CUDA 11.8 support as follows:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n",(0,r.jsx)(n.h2,{id:"using-the-gpu-in-sbi-workflows",children:"Using the GPU in SBI Workflows"}),"\n",(0,r.jsx)(n.p,{children:"PyTorch automatically places tensors and models on the CPU by default. To utilize a GPU, explicitly transfer relevant objects to the CUDA device:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Example: preparing input data\ntheta = theta.to(device)\nx = x.to(device)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Most sbi components are compatible with GPU execution, provided that inputs and models are transferred appropriately."}),"\n",(0,r.jsx)(n.h2,{id:"training-with-gpu",children:"Training with GPU"}),"\n",(0,r.jsx)(n.p,{children:"When training the neural posterior estimator, GPU acceleration is automatically used if:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Input tensors (",(0,r.jsx)(n.code,{children:"theta"}),", ",(0,r.jsx)(n.code,{children:"x"}),") are on the GPU"]}),"\n",(0,r.jsx)(n.li,{children:"The neural network model is on the GPU"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["This process is managed internally by the ",(0,r.jsx)(n.code,{children:"sbi"})," API, but it's good practice to confirm that inputs and outputs remain on the same device."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"inference = SNPE(prior=prior)\ndensity_estimator = inference.append_simulations(theta.to(device), x.to(device)).train()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"batch-size-and-memory",children:"Batch Size and Memory"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Increase batch size to better utilize GPU memory, if available."}),"\n",(0,r.jsx)(n.li,{children:"Monitor GPU usage with nvidia-smi."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"data-movement",children:"Data Movement"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Avoid frequent CPU\u2013GPU transfers, which can bottleneck performance."}),"\n",(0,r.jsx)(n.li,{children:"Pre-load and pre-transform data on the GPU when possible."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"parallelism",children:"Parallelism"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simulation may remain CPU-bound unless the simulator itself is parallelized."}),"\n",(0,r.jsx)(n.li,{children:"If training time dominates, GPU offers the most significant benefit."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"verifying-gpu-usage",children:"Verifying GPU Usage"}),"\n",(0,r.jsx)(n.p,{children:"To confirm that training uses the GPU:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Run nvidia-smi in the terminal during training."}),"\n",(0,r.jsx)(n.li,{children:"Use PyTorch utilities to print device assignment:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"for param in density_estimator.parameters():\n    print(param.device)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"GPU acceleration can dramatically reduce training time for neural density estimators in the SBI pipeline. Ensure all data and models are transferred to the CUDA device, and monitor resource usage to achieve optimal performance."}),"\n",(0,r.jsx)(n.p,{children:"In the next section, we demonstrate a complete inference workflow using a real example."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>s});var t=i(6540);const r={},a=t.createContext(r);function l(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);