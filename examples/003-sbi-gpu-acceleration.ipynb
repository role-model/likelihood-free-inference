{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role R Likelihood-free Simulation Based Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micahtilton/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing the 'roleR' R package from GitHub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from roler.model import ModelPrior\n",
    "from roler.distributions import *\n",
    "from roler.simulation import Simulator\n",
    "\n",
    "prior = ModelPrior(\n",
    "    individuals_local=IntDistribution(50, 300),\n",
    "    individuals_meta=IntDistribution(400, 1000),\n",
    "    species_meta=50,\n",
    "    speciation_local=0.05,\n",
    "    speciation_meta=0.05,\n",
    "    extinction_meta=0.05,\n",
    "    env_sigma=0.5,\n",
    "    trait_sigma=1,\n",
    "    comp_sigma=0.5,\n",
    "    dispersal_prob=0.1,\n",
    "    mutation_rate=0.01,\n",
    "    equilib_escape=1,\n",
    "    num_basepairs=250,\n",
    "    init_type='oceanic_island',\n",
    "    niter=2000,\n",
    "    niterTimestep=10\n",
    ")\n",
    "\n",
    "simulator = Simulator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n",
      "Installing the 'roleR' R package from GitHub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Using GitHub PAT from the git credential store.\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n",
      "R[write to console]: Skipping install of 'roleR' from a github remote, the SHA1 (cc6546a1) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from roler.datasets import Dataset\n",
    "\n",
    "dataset = Dataset(simulator=simulator, prior=prior)\n",
    "x, y = dataset.generate_dataset(16*10, select_last_n=10, n_jobs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micahtilton/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/sbi/inference/trainers/npe/npe_base.py:157: UserWarning: Data x has device 'cpu'. Moving x to the data_device 'mps'. Training will proceed on device 'mps'.\n",
      "  theta, x = validate_theta_and_x(\n",
      "/Users/micahtilton/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/sbi/inference/trainers/npe/npe_base.py:157: UserWarning: Parameters theta has device 'cpu'. Moving theta to the data_device 'mps'. Training will proceed on device 'mps'.\n",
      "  theta, x = validate_theta_and_x(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::unique_dim' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 2236df1770800ffea5697b11b0bb0d910b2e59e1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m snpe \u001b[38;5;241m=\u001b[39m SNPE(prior\u001b[38;5;241m=\u001b[39mprior\u001b[38;5;241m.\u001b[39mget_joint_uniform(device\u001b[38;5;241m=\u001b[39mdevice), device\u001b[38;5;241m=\u001b[39mdevice, density_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m density_estimator \u001b[38;5;241m=\u001b[39m \u001b[43msnpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_simulations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m posterior \u001b[38;5;241m=\u001b[39m snpe\u001b[38;5;241m.\u001b[39mbuild_posterior(density_estimator)\n",
      "File \u001b[0;32m~/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/sbi/inference/trainers/npe/npe_base.py:169\u001b[0m, in \u001b[0;36mPosteriorEstimator.append_simulations\u001b[0;34m(self, theta, x, proposal, exclude_invalid_x, data_device)\u001b[0m\n\u001b[1;32m    166\u001b[0m theta \u001b[38;5;241m=\u001b[39m theta[is_valid_x]\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Check for problematic z-scoring\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[43mwarn_if_zscoring_changes_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNPE_C\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m current_round \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_non_atomic_loss\n\u001b[1;32m    174\u001b[0m ):\n\u001b[1;32m    175\u001b[0m     nle_nre_apt_msg_on_invalid_x(\n\u001b[1;32m    176\u001b[0m         num_nans,\n\u001b[1;32m    177\u001b[0m         num_infs,\n\u001b[1;32m    178\u001b[0m         exclude_invalid_x,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiround SNPE-C (atomic)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    180\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/sbi/utils/sbiutils.py:40\u001b[0m, in \u001b[0;36mwarn_if_zscoring_changes_data\u001b[0;34m(x, duplicate_tolerance)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raise warning if z-scoring would create duplicate data points.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    x: Simulated data.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    duplicate_tolerance: Tolerated proportion of duplicates after z-scoring.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Count unique xs.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m num_unique \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Check we do have different data in the batch\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_unique \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/torch/functional.py:1080\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m-> 1080\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/GitHub/likelihood-free-inference/.venv/lib/python3.12/site-packages/torch/functional.py:965\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    955\u001b[0m         unique,\n\u001b[1;32m    956\u001b[0m         (\u001b[38;5;28minput\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    961\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_dim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    973\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_unique2(\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    975\u001b[0m         \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m,\n\u001b[1;32m    976\u001b[0m         return_inverse\u001b[38;5;241m=\u001b[39mreturn_inverse,\n\u001b[1;32m    977\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    978\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::unique_dim' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 2236df1770800ffea5697b11b0bb0d910b2e59e1. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "from sbi.inference import SNPE\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "snpe = SNPE(prior=prior.get_joint_uniform(device=device), device=device, density_estimator=\"maf\")\n",
    "density_estimator = snpe.append_simulations(x, y).train()\n",
    "posterior = snpe.build_posterior(density_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
